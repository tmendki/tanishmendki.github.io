<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>research · tanish mendki</title>
  <link rel="stylesheet" href="style.css" />
  <style>
    /* minimal extras specific to this page */
    .project-hdr { display: flex; align-items: center; gap: .65rem; font-weight: 600; }
    .icon { width: 20px; aspect-ratio: 1; fill: var(--accent); flex-shrink: 0; }
    .bullets { margin-top: .6rem; padding-left: 1.1rem; list-style: disc; }
    .bullets li + li { margin-top: .45rem; }
  </style>
</head>
<body>

<header>
  <div class="brand">tanish mendki</div>

  <nav>
    <a href="index.html">home</a>
    <a href="about.html">about</a>
    <a href="research.html">research</a>
    <a href="projects.html">projects</a>
    <a href="experience.html">experience</a>
    <a href="contact.html">contact</a>
  </nav>

  <div class="social">
    <!-- spotify -->
    <a href="https://open.spotify.com/user/tanish" target="_blank" aria-label="spotify">
      <svg class="icon" viewBox="0 0 168 168" role="img">
        <path d="M84 0a84 84 0 100 168 84 84 0 000-168zm38.3 120.4c-1.4 2.4-4.4 3.1-6.8 1.7-18.7-11.5-42.2-14.1-70-7.6-2.7.6-5.4-1-6.1-3.7-.7-2.7 1-5.4 3.7-6.1 30.8-7 56.6-4.1 77.2 8.6 2.3 1.4 3 4.4 1.7 6.9zm9.7-20.4c-1.8 2.9-5.6 3.9-8.5 2.1-21.5-13.2-54.2-17-79.6-9.2-3.3 1-6.8-.9-7.8-4.2-1-3.3.9-6.8 4.2-7.8 30.3-9.2 66.7-5 91.5 10.4 2.9 1.8 3.9 5.6 2.1 8.7zm.8-22.4C107.9 64 67.4 62.7 41 70c-3.9 1.1-7.9-1.2-9-5.1-1.1-3.9 1.2-7.9 5.1-9C68.4 47 114.2 48.4 140.2 65.3c3.3 2.1 4.3 6.5 2.1 9.8z"/>
      </svg>
    </a>
    <!-- linkedin -->
    <a href="https://www.linkedin.com/in/tanishmendki" target="_blank" aria-label="linkedin">
      <svg class="icon" viewBox="0 0 448 512" role="img">
        <path d="M100.3 448H7V148.9h93.3V448zM53.7 108.1C24.1 108.1 0 83.5 0 53.9 0 24.3 24.6 0 54.4 0s54.4 24.3 54.4 53.9c-.1 29.6-24.5 54.2-55.1 54.2zM447.9 448h-93V302.4c0-34.7-.7-79.2-48.1-79.2-48.2 0-55.6 37.7-55.6 76.6V448h-93.1V148.9h89.5v40.8h1.3c12.5-23.7 43-48.7 88.5-48.7 94.7 0 112.1 62.4 112.1 143.3V448z"/>
      </svg>
    </a>
  </div>
</header>

<section>
  <h2>current research</h2>

  <ul>
    <!-- background distractors project -->
    <li class="card">
      <div class="project-hdr">
        <!-- target icon -->
        <svg class="icon" viewBox="0 0 24 24"><circle cx="12" cy="12" r="9" stroke="none"/><circle cx="12" cy="12" r="5"/></svg>
        background distractors in visual search
      </div>
      <ul class="bullets">
        <li>designed and implemented a synthetic dataset pipeline simulating <b>structured visual environments</b></li>
        <li>generated images with systematically varied background noise and target positions</li>
        <li>trained & fine‑tuned multiple models, reaching <b>90 % accuracy—on par with humans</b></li>
        <li>probed model representations via targeted manipulations of visual features</li>
        <li>discovered that models <b>rely heavily on background structure</b> to decide</li>
        <li>leveraged pytorch & scientific python to build efficient batch pipelines and visualisations</li>
      </ul>
    </li>

    <!-- vintage vision models project -->
    <li class="card">
      <div class="project-hdr">
        <!-- brain icon -->
        <svg class="icon" viewBox="0 0 24 24"><path d="M9 3a3 3 0 00-3 3v1H5a3 3 0 000 6h1v2H5a3 3 0 000 6h1v1a3 3 0 003 3 3 3 0 006 0 3 3 0 003-3v-1h1a3 3 0 000-6h-1v-2h1a3 3 0 000-6h-1V6a3 3 0 00-3-3 3 3 0 10-6 0z"/></svg>
        are old vision models useless?
      </div>
      <ul class="bullets">
        <li>re‑evaluating whether modern dnns are truly more “brain‑aligned” than classical models</li>
        <li>implemented & trained a suite of <b>legacy mathematical vision models</b></li>
        <li>collected layer‑wise activations on a curated image set</li>
        <li>systematically comparing activations to neural recordings, layer by layer</li>
        <li><b>early results:</b> several classical models remain surprisingly competitive</li>
      </ul>
    </li>

    <!-- deceptive descriptions project -->
    <li class="card">
      <div class="project-hdr">
        <!-- mask icon -->
        <svg class="icon" viewBox="0 0 24 24"><path d="M12 2a9 9 0 00-9 9v3c0 5 4 8 9 8s9-3 9-8v-3a9 9 0 00-9-9zm0 14c-2.3 0-4-.7-5.2-2.1a1 1 0 011.5-1.3c.8.9 2 1.4 3.7 1.4s2.9-.5 3.7-1.4a1 1 0 011.5 1.3C16 15.3 14.3 16 12 16zm-3-6a1.5 1.5 0 110-3 1.5 1.5 0 010 3zm6 0a1.5 1.5 0 110-3 1.5 1.5 0 010 3z"/></svg>
        deceptive descriptions
      </div>
      <ul class="bullets">
        <li>exploring how humans and llms handle <b>fake knowledgeability</b> in everyday captions</li>
        <li>collected deceptive descriptions for <b>360 images</b> across diverse categories</li>
        <li>gathered human ratings of deceptiveness & compared them to llm judgments</li>
        <li>ongoing experiment tests decision‑making when participants face deception</li>
      </ul>
    </li>
  </ul>

  <h2>posters & presentations</h2>
  <ul>
    <li class="card">
      mendki, t. & eckstein, m. (2025). <em>humans and neural networks learn to use background structures for efficient visual search.</em> <span>urca poster</span>
    </li>
    <li class="card">
      mendki, t. & eckstein, m. (2025). <em>humans and neural networks learn to use background structures for efficient visual search.</em> <span>urca oral talk</span>
    </li>
  </ul>
</section>

<footer>© 2025 tanish mendki</footer>

</body>
</html>
