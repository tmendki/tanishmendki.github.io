<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>research · tanish mendki</title>
  <link rel="stylesheet" href="style.css" />

  <!-- page‑local enhancements (identical look‑&‑feel as experience page) -->
  <style>
    .res-card {
      display: block;
      padding: 1rem 1rem 1.1rem;
      margin: 1rem 0 1.2rem;
      border-inline-start: 4px solid var(--accent);
      background: #fafafa;
      border-radius: .6rem;
      box-shadow: 0 0 0 rgba(0,0,0,0);
      transition: box-shadow .25s ease, transform .25s ease;
    }
    .res-card:hover { box-shadow: 0 10px 24px rgba(0,0,0,.08); transform: translateY(-2px); }

    .project-hdr { display: flex; align-items: center; gap: .65rem; font-weight: 700; }
    .icon { width: 22px; aspect-ratio: 1; fill: var(--accent); flex-shrink: 0; }

    .bullets { margin-top: .6rem; padding-left: 1.1rem; list-style: disc; }
    .bullets li + li { margin-top: .45rem; }
  </style>
</head>
<body>

<header>
  <div class="brand">tanish mendki</div>

  <nav>
    <a href="index.html">home</a>
    <a href="about.html">about</a>
    <a href="research.html">research</a>
    <a href="projects.html">projects</a>
    <a href="experience.html">experience</a>
    <a href="contact.html">contact</a>
  </nav>

  <div class="social">
    <a href="https://open.spotify.com/user/tanish" target="_blank" aria-label="spotify">
      <svg class="icon" viewBox="0 0 168 168" role="img"><path d="M84 0a84 84 0 100 168 84 84 0 000-168zm38.3 120.4c-1.4 2.4-4.4 3.1-6.8 1.7-18.7-11.5-42.2-14.1-70-7.6-2.7.6-5.4-1-6.1-3.7-.7-2.7 1-5.4 3.7-6.1 30.8-7 56.6-4.1 77.2 8.6 2.3 1.4 3 4.4 1.7 6.9zm9.7-20.4c-1.8 2.9-5.6 3.9-8.5 2.1-21.5-13.2-54.2-17-79.6-9.2-3.3 1-6.8-.9-7.8-4.2-1-3.3.9-6.8 4.2-7.8 30.3-9.2 66.7-5 91.5 10.4 2.9 1.8 3.9 5.6 2.1 8.7zm.8-22.4C107.9 64 67.4 62.7 41 70c-3.9 1.1-7.9-1.2-9-5.1-1.1-3.9 1.2-7.9 5.1-9C68.4 47 114.2 48.4 140.2 65.3c3.3 2.1 4.3 6.5 2.1 9.8z"/></svg>
    </a>
    <a href="https://www.linkedin.com/in/tanishmendki" target="_blank" aria-label="linkedin">
      <svg class="icon" viewBox="0 0 448 512" role="img"><path d="M100.3 448H7V148.9h93.3V448zM53.7 108.1C24.1 108.1 0 83.5 0 53.9 0 24.3 24.6 0 54.4 0s54.4 24.3 54.4 53.9c-.1 29.6-24.5 54.2-55.1 54.2zM447.9 448h-93V302.4c0-34.7-.7-79.2-48.1-79.2-48.2 0-55.6 37.7-55.6 76.6V448h-93.1V148.9h89.5v40.8h1.3c12.5-23.7 43-48.7 88.5-48.7 94.7 0 112.1 62.4 112.1 143.3V448z"/></svg>
    </a>
  </div>
</header>

<section>
  <h2>current research</h2>

  <!-- background distractors project -->
  <article class="res-card">
    <div class="project-hdr">
      <svg class="icon" viewBox="0 0 24 24"><circle cx="12" cy="12" r="9"/><circle cx="12" cy="12" r="5"/></svg>
      background distractors in visual search
    </div>
    <ul class="bullets">
      <li>built a synthetic dataset generator simulating <b>structured visual environments</b></li>
      <li>varied background noise & target positions across tens of thousands of images</li>
      <li>trained & fine‑tuned models to <b>90 % human‑level accuracy</b></li>
      <li>discovered models <b>over‑rely on background cues</b>—insightful for medical‑imaging safety</li>
      <li>pipeline implemented in pytorch, accelerated with multiprocessing & on‑GPU augmentations</li>
    </ul>
  </article>

  <!-- vintage vision models project -->
  <article class="res-card">
    <div class="project-hdr">
      <svg class="icon" viewBox="0 0 24 24"><path d="M9 3a3 3 0 00-3 3v1H5a3 3 0 000 6h1v2H5a3 3 0 000 6h1v1a3 3 0 003 3 3 3 0 006 0 3 3 0 003-3v-1h1a3 3 0 000-6h-1v-2h1a3 3 0 000-6h-1V6a3 3 0 00-3-3 3 3 0 10-6 0z"/></svg>
      are old vision models useless?
    </div>
    <ul class="bullets">
      <li>re‑implemented <b>classical vision models</b> (e.g., gabor pyramids, gist, hmax) from scratch</li>
      <li>ran a curated benchmark of natural & synthetic stimuli through each architecture</li>
      <li>extracted layer‑wise activations and compared them to primate neural data</li>
      <li><b>surprise finding:</b> certain hand‑crafted features remain highly brain‑aligned</li>
      <li>paper draft in prep; tool released under mit license for community replication</li>
    </ul>
  </article>

  <!-- deceptive descriptions project -->
  <article class="res-card">
    <div class="project-hdr">
      <svg class="icon" viewBox="0 0 24 24"><path d="M12 2a9 9 0 00-9 9v3c0 5 4 8 9 8s9-3 9-8v-3a9 9 0 00-9-9zm0 14c-2.3 0-4-.7-5.2-2.1a1 1 0 011.5-1.3c.8.9 2 1.4 3.7 1.4s2.9-.5 3.7-1.4a1 1 0 011.5 1.3C16 15.3 14.3 16 12 16zm-3-6a1.5 1.5 0 110-3 1.5 1.5 0 010 3zm6 0a1.5 1.5 0 110-3 1.5 1.5 0 010 3z"/></svg>
      deceptive descriptions
    </div>
    <ul class="bullets">
      <li>studying how humans and llms rate <b>“fake expertise”</b> in everyday image captions</li>
      <li>curated 360 images with intentionally misleading descriptions across 5 domains</li>
      <li>collected human deceptiveness ratings; compared to gpt‑4 & llama‑3 judgments</li>
      <li>running a follow‑up to test decision‑making under deceptive information</li>
    </ul>
  </article>

  <h2>posters & presentations</h2>

  <article class="res-card">
    mendki, t. & eckstein, m. (2025). <em>humans and neural networks learn to use background structures for efficient visual search.</em> <span>urca poster & oral talk</span>
  </article>
</section>

<footer>© 2025 tanish mendki</footer>

</body>
</html>
